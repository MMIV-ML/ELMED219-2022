{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ELMED219-2022_. Alexander S. Lundervold, 09.01.2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing and machine learning: a small case-study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick example of some techniques and ideas from natural language processing (NLP) and some approaches to NLP based on _deep learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: we'll take a close look at what deep learning is in tomorrow's lecture and lab.\n",
    "\n",
    "> Note: If you want to run this notebook on your own computer, ask Alexander for assistance. The software requirements are different from the other ELMED219 notebooks (and also slightly more tricky to install, depending on your setup). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** If you're running the notebook on Colab, you should attach a GPU to the session by clicking \"Runtime\" --> \"Change runtime type\" and then selecting a GPU hardware accelerator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is not running on Colab. colab=False.\n"
     ]
    }
   ],
   "source": [
    "# This is a quick check of whether the notebook is currently running \n",
    "# on Google Colaboratory\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('The notebook is running on Colab. colab=True.')\n",
    "    colab=True\n",
    "else:\n",
    "    print('The notebook is not running on Colab. colab=False.')\n",
    "    colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: To run this notebook locally you will have to install fastai\n",
      "See the course repo for details\n"
     ]
    }
   ],
   "source": [
    "if colab:\n",
    "    import sys\n",
    "    # Install fastai etc\n",
    "    !pip install -Uqq fastbook\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    from fastbook import *\n",
    "if not colab:\n",
    "    print(\"WARNING: To run this notebook locally you will have to install fastai\")\n",
    "    print(\"See the course repo for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [spacy library]() for NLP and the [fastai]() library for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a data set collected in the work of Wakamiya et.al, _Tweet Classification Toward Twitter-Based Disease Surveillance: New Data, Methods, and Evaluations_, 2019: https://www.jmir.org/2019/2/e12783/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![medweb-paper](https://github.com/MMIV-ML/ELMED219-2022/raw/main/Lab2-NLP/assets/medweb-paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data us supposed to represent tweets that discusses one or more of eight symptoms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the original paper:\n",
    "<img src=\"https://github.com/MMIV-ML/ELMED219-2022/raw/main/Lab2-NLP/assets/medweb_examples.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only look at the English language tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/MMIV-ML/ELMED219-2022/raw/main/Lab2-NLP/data/medwebdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Influenza</th>\n",
       "      <th>Diarrhea</th>\n",
       "      <th>Hayfever</th>\n",
       "      <th>Cough</th>\n",
       "      <th>Headache</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Runnynose</th>\n",
       "      <th>Cold</th>\n",
       "      <th>labels</th>\n",
       "      <th>is_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1en</td>\n",
       "      <td>The cold makes my whole body weak.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2en</td>\n",
       "      <td>It's been a while since I've had allergy symptoms.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hayfever;Runnynose</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3en</td>\n",
       "      <td>I'm so feverish and out of it because of my allergies. I'm so sleepy.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hayfever;Fever;Runnynose</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4en</td>\n",
       "      <td>I took some medicine for my runny nose, but it won't stop.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Runnynose</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5en</td>\n",
       "      <td>I had a bad case of diarrhea when I traveled to Nepal.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sober</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                                                  Tweet  \\\n",
       "0  1en                                     The cold makes my whole body weak.   \n",
       "1  2en                     It's been a while since I've had allergy symptoms.   \n",
       "2  3en  I'm so feverish and out of it because of my allergies. I'm so sleepy.   \n",
       "3  4en             I took some medicine for my runny nose, but it won't stop.   \n",
       "4  5en                 I had a bad case of diarrhea when I traveled to Nepal.   \n",
       "\n",
       "   Influenza  Diarrhea  Hayfever  Cough  Headache  Fever  Runnynose  Cold  \\\n",
       "0          0         0         0      0         0      0          0     1   \n",
       "1          0         0         1      0         0      0          1     0   \n",
       "2          0         0         1      0         0      1          1     0   \n",
       "3          0         0         0      0         0      0          1     0   \n",
       "4          0         0         0      0         0      0          0     0   \n",
       "\n",
       "                     labels  is_test  \n",
       "0                      Cold    False  \n",
       "1        Hayfever;Runnynose    False  \n",
       "2  Hayfever;Fever;Runnynose    False  \n",
       "3                 Runnynose    False  \n",
       "4                     sober    False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"They say we will have less pollen next spring, but it doesn't really matter \"\n",
      " 'to me, since my allergy gets severe in the autumn.')\n"
     ]
    }
   ],
   "source": [
    "pp(df['Tweet'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this text the goal is to determine whether the person is talking about one or more of the eight symptoms or conditions listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Influenza',\n",
       " 'Diarrhea',\n",
       " 'Hayfever',\n",
       " 'Cough',\n",
       " 'Headache',\n",
       " 'Fever',\n",
       " 'Runnynose',\n",
       " 'Cold']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns[2:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **BUT:** How can a computer read??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://2.bp.blogspot.com/_--uVHetkUIQ/TDae5jGna8I/AAAAAAAAAK0/sBSpLudWmcw/s1600/reading.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a computer, everything is numbers. We have to convert the text to a series of numbers, and then feed those to the computer. \n",
    "\n",
    "This can be done in two widely used steps in natural language processing: **tokenization** and **numericalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tokenization the text is split into single words, called tokens. A simple way to achieve this is to split according to spaces in the text. But then we, among other things, lose punctuation, and also the fact that some words are contractions of multiple words (for example _isn't_ and _don't_). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some result after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj i 'm so tired from coughing so much . xxbos xxmaj the fever medicine xxmaj xxunk gave me works crazy well , i got over my cold in no time xxbos i put on a mask and am xxunk with headache medicine . xxbos i have to get xxunk for allergy season ! xxbos xxmaj i 've always had all xxunk of xxunk , like allergies and eczema . i</td>\n",
       "      <td>xxmaj i 'm so tired from coughing so much . xxbos xxmaj the fever medicine xxmaj xxunk gave me works crazy well , i got over my cold in no time xxbos i put on a mask and am xxunk with headache medicine . xxbos i have to get xxunk for allergy season ! xxbos xxmaj i 've always had all xxunk of xxunk , like allergies and eczema . i have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>headache . xxmaj this is bad . xxbos i wonder if i caught a bug since i have a high fever . xxbos i got check out at the hospital because i was n't feeling well , and it turned out to be the flu . xxbos xxmaj they say a cold is not a disease . xxbos xxmaj this is bad , xxmaj xxunk has a cold . xxbos xxmaj by</td>\n",
       "      <td>. xxmaj this is bad . xxbos i wonder if i caught a bug since i have a high fever . xxbos i got check out at the hospital because i was n't feeling well , and it turned out to be the flu . xxbos xxmaj they say a cold is not a disease . xxbos xxmaj this is bad , xxmaj xxunk has a cold . xxbos xxmaj by the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextDataLoaders.from_df(df, text_col='Tweet', is_lm=True, valid_pct=0.1)\n",
    "\n",
    "data_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens starting with \"xx\" are special. `xxbos` means the beginning of the text, `xxmaj` means that the following word is capitalized, `xxup` means that the following word is in all caps, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens `xxunk` replaces words that are rare in the text corpus. We keep only words that appear at least twice (with a set maximum number of different words, 60.000 in our case). This is called our **vocabulary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert tokens to numbers by making a list of all the tokens that have been used and assign them to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above text is replaced by numbers, as in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([  2,   8,  13, 140,  19, 187,  10,   8, 121,  53,  11, 228,  28,  17,\n",
       "         35,  10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **We are now in a position where the computer can compute on the text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Classical\" versus deep learning-based NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection: splitting into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentence:\n",
    "> _\"Patient presents for initial evaluation of cough. Cough is reported to have developed acutely and has been present for 4 days. Symptom severity is moderate. Will return next week.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient presents for initial evaluation of cough.\n",
      "Cough is reported to have developed acutely and has been present for 4 days.\n",
      "Symptom severity is moderate.\n",
      "Will return next week.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Patient presents for initial evaluation of cough. Cough is reported to have developed acutely and has been present for 4 days. Symptom severity is moderate. Will return next week.\"\n",
    "doc = nlp(sentence)\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 days DATE\n",
      "Symptom PERSON\n",
      "next week DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Patient presents for initial evaluation of cough. Cough is reported to have developed acutely and has been present for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Symptom\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " severity is moderate. Will return \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    next week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f4185ae8ed924283bc6ec6dd53b5f178-0\" class=\"displacy\" width=\"2660\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Patient</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">presents</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">initial</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">evaluation</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">cough.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">Cough</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">reported</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">developed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1220\">acutely</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1220\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">been</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1580\">present</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1580\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1670\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1670\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1760\">4</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1760\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">days.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1940\">Symptom</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1940\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2030\">severity</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2030\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2120\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2120\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2210\">moderate.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2210\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2300\">Will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2300\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2390\">return</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2390\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2480\">next</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2480\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2570\">week.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2570\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,137.0 125.0,137.0 125.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-1\" stroke-width=\"2px\" d=\"M160,182.0 C160,137.0 215.0,137.0 215.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M215.0,184.0 L223.0,172.0 207.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-2\" stroke-width=\"2px\" d=\"M340,182.0 C340,137.0 395.0,137.0 395.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,184.0 L332,172.0 348,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-3\" stroke-width=\"2px\" d=\"M250,182.0 C250,92.0 400.0,92.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,184.0 L408.0,172.0 392.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-4\" stroke-width=\"2px\" d=\"M430,182.0 C430,137.0 485.0,137.0 485.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M485.0,184.0 L493.0,172.0 477.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-5\" stroke-width=\"2px\" d=\"M520,182.0 C520,137.0 575.0,137.0 575.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,184.0 L583.0,172.0 567.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-6\" stroke-width=\"2px\" d=\"M700,182.0 C700,92.0 850.0,92.0 850.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M700,184.0 L692,172.0 708,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-7\" stroke-width=\"2px\" d=\"M790,182.0 C790,137.0 845.0,137.0 845.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,184.0 L782,172.0 798,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-8\" stroke-width=\"2px\" d=\"M970,182.0 C970,92.0 1120.0,92.0 1120.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,184.0 L962,172.0 978,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-9\" stroke-width=\"2px\" d=\"M1060,182.0 C1060,137.0 1115.0,137.0 1115.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060,184.0 L1052,172.0 1068,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-10\" stroke-width=\"2px\" d=\"M880,182.0 C880,47.0 1125.0,47.0 1125.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1125.0,184.0 L1133.0,172.0 1117.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-11\" stroke-width=\"2px\" d=\"M1150,182.0 C1150,137.0 1205.0,137.0 1205.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1205.0,184.0 L1213.0,172.0 1197.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-12\" stroke-width=\"2px\" d=\"M1150,182.0 C1150,92.0 1300.0,92.0 1300.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1300.0,184.0 L1308.0,172.0 1292.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-13\" stroke-width=\"2px\" d=\"M1420,182.0 C1420,137.0 1475.0,137.0 1475.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1420,184.0 L1412,172.0 1428,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-14\" stroke-width=\"2px\" d=\"M1150,182.0 C1150,2.0 1490.0,2.0 1490.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1490.0,184.0 L1498.0,172.0 1482.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-15\" stroke-width=\"2px\" d=\"M1510,182.0 C1510,137.0 1565.0,137.0 1565.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1565.0,184.0 L1573.0,172.0 1557.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-16\" stroke-width=\"2px\" d=\"M1510,182.0 C1510,92.0 1660.0,92.0 1660.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1660.0,184.0 L1668.0,172.0 1652.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-17\" stroke-width=\"2px\" d=\"M1780,182.0 C1780,137.0 1835.0,137.0 1835.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780,184.0 L1772,172.0 1788,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-18\" stroke-width=\"2px\" d=\"M1690,182.0 C1690,92.0 1840.0,92.0 1840.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1840.0,184.0 L1848.0,172.0 1832.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-19\" stroke-width=\"2px\" d=\"M1960,182.0 C1960,137.0 2015.0,137.0 2015.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1960,184.0 L1952,172.0 1968,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-20\" stroke-width=\"2px\" d=\"M2050,182.0 C2050,137.0 2105.0,137.0 2105.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2050,184.0 L2042,172.0 2058,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-21\" stroke-width=\"2px\" d=\"M2140,182.0 C2140,137.0 2195.0,137.0 2195.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2195.0,184.0 L2203.0,172.0 2187.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-22\" stroke-width=\"2px\" d=\"M2320,182.0 C2320,137.0 2375.0,137.0 2375.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2320,184.0 L2312,172.0 2328,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-23\" stroke-width=\"2px\" d=\"M2500,182.0 C2500,137.0 2555.0,137.0 2555.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2500,184.0 L2492,172.0 2508,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-24\" stroke-width=\"2px\" d=\"M2410,182.0 C2410,92.0 2560.0,92.0 2560.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4185ae8ed924283bc6ec6dd53b5f178-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2560.0,184.0 L2568.0,172.0 2552.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's a lot more to natural language processing, of course! Have a look at [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101) for some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, data preparation and feature engineering is a huge and difficult undertaking when using machine learning to analyse text. \n",
    "\n",
    "However, in what's called _deep learning_ (discussed in detail tomorrow) most of this work is done by the computer! That's because deep learning does feature extraction _and_ prediction in the same model. \n",
    "\n",
    "This results in much less work and, often, _in much better models_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLvsDL](https://aylien.com/images/uploads/general/tumblr_inline_oabas5sThb1sleek4_540.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now come to a relatively new and very powerful idea for deep learning and NLP. An idea that created a small revolution in NLP a couple of years ago ([1](https://blog.openai.com/language-unsupervised/), [2](http://ruder.io/nlp-imagenet/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a system that can classify text into one or more categories. This is a difficult problem as the computer must somehow implicitly learn to \"read\". \n",
    "\n",
    "Idea: why not _first_ teach the computer to \"read\" and _then_ let it loose on the classification task?\n",
    "\n",
    "We can teach the computer to \"understand\" language by training it to predict the next word of a sentence, using as much training data we can get hold of. This is called ***language modelling*** in NLP. \n",
    "\n",
    "This is a difficult task: to guess the next word of a sentence one has to know a lot about language, and also a lot about the world.\n",
    "\n",
    "> What word fits here? _\"The light turned green and Per crossed the ___\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, obtaining large amounts of training data for language models is simple: any text can be used. The labels are simply the next word of a subpart of the text. \n",
    "\n",
    "We can for example use Wikipedia. After the model performs alright at predicting the next word of Wikipedia text, we can fine-tune it on text that's closer to the classification task we're after. \n",
    "\n",
    "> This is often called ***transfer learning***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tweet text to fine-tune a model that's already been pretrained on Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj xxunk , i was n't feeling well last night , turns out it 's the flu . xxbos xxmaj i 'm taking medicine for this headache . xxbos xxmaj maybe not a lot of people know this , but there are fall allergies too ! xxbos xxmaj it was just a cold and a runny nose , but now my head is starting to hurt , so xxmaj i 'm</td>\n",
       "      <td>xxmaj xxunk , i was n't feeling well last night , turns out it 's the flu . xxbos xxmaj i 'm taking medicine for this headache . xxbos xxmaj maybe not a lot of people know this , but there are fall allergies too ! xxbos xxmaj it was just a cold and a runny nose , but now my head is starting to hurt , so xxmaj i 'm gon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i was coughing up , and it turned out that i had a high fever . xxbos xxmaj lately i have xxunk mild headaches . xxbos i wear a mask as a xxunk against the flu when i go out . xxbos xxmaj my abs hurt a lot from coughing xxunk . xxbos i have school but xxmaj i 'll stay in bed because i have a slight headache . xxbos i</td>\n",
       "      <td>was coughing up , and it turned out that i had a high fever . xxbos xxmaj lately i have xxunk mild headaches . xxbos i wear a mask as a xxunk against the flu when i go out . xxbos xxmaj my abs hurt a lot from coughing xxunk . xxbos i have school but xxmaj i 'll stay in bed because i have a slight headache . xxbos i have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought it was a cold , but it turned out to be allergies . i had no idea it could cause a fever as well . xxbos i never thought i would suffer from allergies . xxbos i walk so xxunk when i have a headache , so that way , it does n't hurt as much . xxbos xxmaj my allergies are finally back . xxmaj it 's been quite a</td>\n",
       "      <td>it was a cold , but it turned out to be allergies . i had no idea it could cause a fever as well . xxbos i never thought i would suffer from allergies . xxbos i walk so xxunk when i have a headache , so that way , it does n't hurt as much . xxbos xxmaj my allergies are finally back . xxmaj it 's been quite a while</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextDataLoaders.from_df(df, text_col='Tweet', is_lm=True, valid_pct=0.1)\n",
    "\n",
    "data_lm.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, pretrained=True, drop_mult=0.3,\n",
    "                               metrics=[accuracy, Perplexity()], wd=0.1, model_dir='.').to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.856825</td>\n",
       "      <td>4.581619</td>\n",
       "      <td>0.246830</td>\n",
       "      <td>97.672424</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.637570</td>\n",
       "      <td>2.974067</td>\n",
       "      <td>0.372962</td>\n",
       "      <td>19.571363</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.097656</td>\n",
       "      <td>2.602711</td>\n",
       "      <td>0.422554</td>\n",
       "      <td>13.500284</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.737985</td>\n",
       "      <td>2.581139</td>\n",
       "      <td>0.425272</td>\n",
       "      <td>13.212176</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and save the parts of the model that we can reuse for classification later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('medweb_finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the language model by having it guess the next given number of words on a starting text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text(seed_text, nb_words):\n",
    "    \"\"\"\n",
    "    Use the trained language model to produce text. \n",
    "    Input:\n",
    "        seed_text: some text to get the model started\n",
    "        nb_words: number of words to produce\n",
    "    \"\"\"\n",
    "    pred = learn.predict(seed_text, nb_words, temperature=0.75)\n",
    "    pp(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I 'm not feeling too good as my nose has hurt , so i do n't want to\"\n"
     ]
    }
   ],
   "source": [
    "make_text(\"I'm not feeling too good as my\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"No , that 's a bug getting it out of my mouth , it 's really bad , and my \"\n",
      " \"nose is so feverish . It 's so bad . My wife has a fever and is not sure if \"\n",
      " \"it 's\")\n"
     ]
    }
   ],
   "source": [
    "make_text(\"No, that's a\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have something that seems to produce text that resembles the text to be classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** It's interesting to see that the model can come up with text that makes some sense (mostly thanks to training on Wikipedia), and that the text resembles the medical tweets (thanks to the fine-tuning). \n",
    "\n",
    "> **Note** also that an accuracy of 30-40% when predicting the next word of a sentence is pretty impressive, as the number of possibilities is very large (equal to the size of the vocabulary).\n",
    "\n",
    "> **Also note** that this is not the task we care about: it's a pretext task before the tweet classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "medweb = DataBlock(blocks=(TextBlock.from_df(text_cols='Tweet', seq_len=12, vocab=data_lm.vocab), MultiCategoryBlock), \n",
    "                  get_x = ColReader(cols='text'), \n",
    "                  get_y = ColReader(cols='labels', label_delim=\";\"),\n",
    "                  splitter = ColSplitter(col='is_test'))\n",
    "\n",
    "data = medweb.dataloaders(df, bs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our task is to predict the possible classes the tweets can be assigned to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i heard that in xxmaj china , someone died of a new type of flu xxunk from the bird flu . xxmaj that got me wondering if there is any xxunk like some xxunk to check to see if i have the flu or not . xxmaj they talk about the flu a lot on xxup tv , but i have no idea how they xxunk the outbreak .</td>\n",
       "      <td>sober</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj not only do i have a runny nose today , but i also have a stuffy nose . xxmaj i 've had to breathe through my mouth , and now my mouth is crazy dry . i have an important meeting tomorrow that i ca n't miss . xxmaj this is so , so bad .</td>\n",
       "      <td>Runnynose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj you and i both have colds . i do n't have any fever medicine . xxmaj there 's a meeting at the xxunk tomorrow . i want to go to the doctor , but even with all the scientific xxunk , there 's no medicine that will cure you in an xxunk .</td>\n",
       "      <td>Cold;Fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos i read some of the how - to xxunk on nasal xxunk , but xxunk of them was xxunk xxunk . i also see so many xxunk personal remedies on xxunk . i think i should just go to the doctor 's and have them take a look .</td>\n",
       "      <td>Runnynose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj my stuffy nose is finally gone , so i went for a xxmaj tokyo xxunk . xxmaj it 's great how you can see the xxunk you love while out on a walk . xxmaj like pretty xxunk , or a xxunk talking with their child .</td>\n",
       "      <td>sober</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj tokyo is cool these days - in xxunk , xxmaj i 'd say it 's cold . xxmaj it 's weird , it 's the xxunk . xxmaj people i know are catching colds and getting fevers , maybe because of this strange weather .</td>\n",
       "      <td>sober</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj god . xxmaj it 's been a while since xxmaj i 've had such a xxunk headache . xxmaj it might be the kind that you 're not supposed to take a bath with . xxmaj it made my headache worse .</td>\n",
       "      <td>Headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj my nose wo n't stop running and my stomach feels xxunk . xxmaj if i get a fever to xxunk , i do n't know if xxmaj i 'll be able to go on the trip this weekend .</td>\n",
       "      <td>Diarrhea;Runnynose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clf = text_classifier_learner(data, AWD_LSTM, seq_len=16, pretrained=True, \n",
    "                                    drop_mult=0.5, \n",
    "                                    metrics=[accuracy_multi, \n",
    "                                             F1ScoreMulti(average='micro'),\n",
    "                                             F1ScoreMulti(average='macro'),\n",
    "                                             PrecisionMulti(average='micro'),\n",
    "                                             PrecisionMulti(average='macro'),\n",
    "                                             RecallMulti(average='micro'),\n",
    "                                             RecallMulti(average='macro'),\n",
    "                                             HammingLossMulti(),\n",
    "                                             ], \n",
    "                                    model_dir='.').to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clf = learn_clf.load_encoder('medweb_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>hamming_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.375475</td>\n",
       "      <td>0.294220</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.466102</td>\n",
       "      <td>0.414160</td>\n",
       "      <td>0.708763</td>\n",
       "      <td>0.783236</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.307227</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>hamming_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.277570</td>\n",
       "      <td>0.218796</td>\n",
       "      <td>0.915451</td>\n",
       "      <td>0.633007</td>\n",
       "      <td>0.626768</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.774737</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.544901</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212202</td>\n",
       "      <td>0.185644</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.726158</td>\n",
       "      <td>0.743580</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.787439</td>\n",
       "      <td>0.672980</td>\n",
       "      <td>0.757399</td>\n",
       "      <td>0.069792</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179836</td>\n",
       "      <td>0.137869</td>\n",
       "      <td>0.946527</td>\n",
       "      <td>0.795485</td>\n",
       "      <td>0.800636</td>\n",
       "      <td>0.838936</td>\n",
       "      <td>0.823156</td>\n",
       "      <td>0.756313</td>\n",
       "      <td>0.793021</td>\n",
       "      <td>0.053472</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.165241</td>\n",
       "      <td>0.133466</td>\n",
       "      <td>0.952604</td>\n",
       "      <td>0.821452</td>\n",
       "      <td>0.822977</td>\n",
       "      <td>0.852103</td>\n",
       "      <td>0.844329</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.809654</td>\n",
       "      <td>0.047396</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.141117</td>\n",
       "      <td>0.950694</td>\n",
       "      <td>0.819797</td>\n",
       "      <td>0.820073</td>\n",
       "      <td>0.823980</td>\n",
       "      <td>0.803811</td>\n",
       "      <td>0.815657</td>\n",
       "      <td>0.853039</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.105486</td>\n",
       "      <td>0.126308</td>\n",
       "      <td>0.955382</td>\n",
       "      <td>0.832355</td>\n",
       "      <td>0.828422</td>\n",
       "      <td>0.860999</td>\n",
       "      <td>0.845736</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.826338</td>\n",
       "      <td>0.044618</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.095217</td>\n",
       "      <td>0.136610</td>\n",
       "      <td>0.957465</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.844146</td>\n",
       "      <td>0.862252</td>\n",
       "      <td>0.849713</td>\n",
       "      <td>0.821970</td>\n",
       "      <td>0.845050</td>\n",
       "      <td>0.042535</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.075972</td>\n",
       "      <td>0.128434</td>\n",
       "      <td>0.957292</td>\n",
       "      <td>0.842510</td>\n",
       "      <td>0.842980</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.839478</td>\n",
       "      <td>0.830808</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058423</td>\n",
       "      <td>0.130946</td>\n",
       "      <td>0.961285</td>\n",
       "      <td>0.858771</td>\n",
       "      <td>0.859236</td>\n",
       "      <td>0.861499</td>\n",
       "      <td>0.844787</td>\n",
       "      <td>0.856061</td>\n",
       "      <td>0.884294</td>\n",
       "      <td>0.038715</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>0.963541</td>\n",
       "      <td>0.866071</td>\n",
       "      <td>0.863495</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.858302</td>\n",
       "      <td>0.857323</td>\n",
       "      <td>0.873717</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.045517</td>\n",
       "      <td>0.134866</td>\n",
       "      <td>0.960937</td>\n",
       "      <td>0.856779</td>\n",
       "      <td>0.855812</td>\n",
       "      <td>0.863928</td>\n",
       "      <td>0.844650</td>\n",
       "      <td>0.849747</td>\n",
       "      <td>0.876103</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.043986</td>\n",
       "      <td>0.134475</td>\n",
       "      <td>0.961979</td>\n",
       "      <td>0.861129</td>\n",
       "      <td>0.858780</td>\n",
       "      <td>0.864968</td>\n",
       "      <td>0.844565</td>\n",
       "      <td>0.857323</td>\n",
       "      <td>0.881054</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_clf.fine_tune(12, base_lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it a good classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test it out on some example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((#2) ['Headache','Runnynose'],\n",
       " tensor([False, False, False, False, False,  True, False,  True, False]),\n",
       " tensor([1.1476e-01, 6.4132e-03, 1.1879e-03, 4.8030e-04, 1.7376e-02, 9.9395e-01,\n",
       "         1.5937e-05, 9.4892e-01, 2.6214e-03]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_clf.predict(\"I'm feeling really bad. My head hurts. My nose is runny. I've felt like this for days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to produce reasonable results. _But remember that this is a very small data set._ One cannot expect very great things when asking the model to make predictions on text outside the small material it has been trained on. \n",
    "\n",
    "Go ahead and try to have the model predict symptoms for a few example sentences and you'll see.\n",
    "\n",
    "This illustrates the need for \"big data\" in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it compare to other approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [original article](https://www.jmir.org/2019/2/e12783/) from 2019 that presented the data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MMIV-ML/ELMED219-2022/raw/main/Lab2-NLP/assets/medweb_results.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"NAIST-en\" models are _\"ensembles of hierarchical attention network and deep character-level convolutional neural network with loss functions (negative loss function, hinge, and hinge squared)\"_. I.e. also deep learning-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End notes\n",
    "\n",
    "* This of course only skratches the surface of NLP and deep learning applied to NLP. The goal was to \"lift the curtain\" and show some of the ideas behind modern text analysis software.\n",
    "* If you're interested in digging into deep learning for NLP you should check out [Hugging Face](https://huggingface.co). and also `fastai` (used above).  \n",
    "\n",
    "\n",
    "> The approach taken above is not really the state-of-the-art for NLP anymore. Over the past two-three years so-called **Transformer** models have taken over in NLP. They are for example behind NLP breakthroughs such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3). [Hugging Face](https://huggingface.co) is a great starting point to learn more and get started using Transfomer models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
